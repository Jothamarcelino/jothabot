import os
import pandas as pd
import streamlit as st
from groq import Groq
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores.utils import cosine_similarity

# Inicializa o cliente Groq
client = Groq(api_key=st.secrets["GROQ_API"])

# Fun√ß√£o de carregamento de √≠ndice vetorial com cache
@st.cache_resource(show_spinner=False)
def carregar_retriever(path):
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    )
    return FAISS.load_local(path, embeddings, allow_dangerous_deserialization=True).as_retriever()

# Carrega os retrievers
retriever_faq = carregar_retriever("vectorstore/faq_index")
retriever_pdf = carregar_retriever("vectorstore/legal_index")
retriever_planos = carregar_retriever("vectorstore/planos_index")

# Busca exata no FAQ com similaridade por metadado e filtro por curso
def buscar_faq_exata(pergunta):
    docs = retriever_faq.vectorstore.similarity_search(pergunta, k=10)
    if not docs:
        return None

    curso_usuario = st.session_state.get("curso", "").strip().lower()
    pergunta_embedding = retriever_faq.vectorstore.embedding_function.embed_query(pergunta)

    melhor_doc = None
    melhor_score = -1

    for doc in docs:
        curso_doc = doc.metadata.get("curso", "geral").strip().lower()
        if curso_doc not in ["geral", curso_usuario]:
            continue

        texto_referencia = doc.metadata.get("input", "") or doc.page_content[:200]
        doc_embedding = retriever_faq.vectorstore.embedding_function.embed_query(texto_referencia)
        score = cosine_similarity([pergunta_embedding], [doc_embedding])[0][0]

        if score > melhor_score:
            melhor_doc = doc
            melhor_score = score

    if melhor_score > 0.85:
        return melhor_doc
    return None

# Fun√ß√£o principal de resposta do JOTHA
def responder_usuario(pergunta):
    if not retriever_faq and not retriever_pdf and not retriever_planos:
        return (
            "‚ö†Ô∏è Os arquivos vetoriais ainda n√£o foram carregados. "
            "Acesse a aba 'Files' e envie as pastas `faq_index`, `legal_index` e `planos_index`, depois clique em 'Rerun'.",
            False,
        )

    curso_usuario = st.session_state.get("curso", "").strip().lower()
    contexto_usuario = f"O usu√°rio √© do curso {curso_usuario.replace('_', ' ').title()}.\n" if curso_usuario else ""

    # Tenta responder via FAQ com busca exata
    doc_exato = buscar_faq_exata(pergunta)
    if doc_exato:
        return doc_exato.page_content.strip(), True

    # Recupera documentos dos planos e das leis com base no curso do usu√°rio
    def filtrar_por_curso(docs, fonte):
        filtrados = []
        for doc in docs:
            curso_doc = doc.metadata.get("curso", "geral").strip().lower()
            if curso_doc == "geral" or curso_doc == curso_usuario:
                filtrados.append(doc)
        return filtrados

    docs_faq = filtrar_por_curso(retriever_faq.get_relevant_documents(pergunta), "faq")
    docs_pdf = retriever_pdf.get_relevant_documents(pergunta)
    docs_planos = filtrar_por_curso(retriever_planos.get_relevant_documents(pergunta), "planos")

    if not docs_faq and not docs_pdf and not docs_planos:
        return (
            "ü§î Hmm... n√£o encontrei nada sobre isso nos meus arquivos. Mas j√° registrei sua d√∫vida! üòâ",
            False,
        )

    # Contexto combinado para o prompt
    contexto = "\n\n".join(
        [doc.page_content for doc in (docs_faq[:2] + docs_pdf[:2] + docs_planos[:3])]
    )[:15000]

    historico = ""
    if "chat_history" in st.session_state:
        historico = "\n".join(
            [f"{entry['role']}: {entry['content']}" for entry in st.session_state.chat_history[-6:]]
        )

    prompt = f"""
{contexto_usuario}
Hist√≥rico da conversa:
{historico}

Voc√™ √© o JOTHA, assistente virtual da Coordena√ß√£o de Est√°gio do IF Sudeste MG - Campus Barbacena.
Responda com simpatia, use emoticons e seja direto. Sua resposta deve se basear apenas no contexto abaixo.
‚ö†Ô∏è **N√£o invente informa√ß√µes. N√£o improvise. Seja claro, preciso e divertido.**

Se a resposta estiver no contexto, use exatamente o que estiver escrito.
Se n√£o encontrar a resposta, diga que n√£o encontrou e oriente o usu√°rio a entrar em contato com a Coordena√ß√£o de Est√°gio.

Contexto:
{contexto}

Pergunta:
{pergunta}

Resposta:
"""

    response = client.chat.completions.create(
        model="llama3-8b-8192",
        messages=[
            {"role": "system", "content": "Voc√™ responde em portugu√™s, com gentileza e precis√£o."},
            {"role": "user", "content": prompt},
        ],
        temperature=0.3,
        max_tokens=512,
    )

    return response.choices[0].message.content.strip(), True

# Registro de perguntas n√£o respondidas
def registrar_pergunta_nao_respondida(pergunta):
    os.makedirs("data", exist_ok=True)
    filepath = "data/nao_respondido.csv"

    if not os.path.exists(filepath):
        pd.DataFrame(columns=["pergunta"]).to_csv(filepath, index=False)

    df = pd.read_csv(filepath)
    if pergunta not in df["pergunta"].values:
        df.loc[len(df)] = [pergunta]
        df.to_csv(filepath, index=False)